{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import your libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 -  Explore the Internal Dataset\n",
    "\n",
    "In this lab, we will start off by working with the wine dataset in scikit-learn. We will select the wine dataset and use a clustering algorithm to learn more about the functionalities of this library. \n",
    "\n",
    "We start off by loading the dataset using the `load_wine` function ([documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html)). In the cell below, we will import the function from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, use the `load_wine` function and assign the wine dataset to a variable called `wine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "wine = load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, list the keys of the variable `wine` to examine its contents. Note that the `load_wine` function does not return dataframes. It returns you a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "wine.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, list the feature names. These are the different characteristics of the wine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'malic_acid',\n",
       " 'ash',\n",
       " 'alcalinity_of_ash',\n",
       " 'magnesium',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'nonflavanoid_phenols',\n",
       " 'proanthocyanins',\n",
       " 'color_intensity',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "wine.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the description of the dataset in the cell below using the `DESCR` attribute of the `wine` variable.\n",
    "\n",
    "*Hint: If your output is ill-formatted by displaying linebreaks as `\\n`, it means you are not using the print function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178 (50 in each of three classes)\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here:\n",
    "print(wine.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the description, we see that all columns are numeric. We also know that there is no missing data \n",
    "\n",
    "Let's plot the alcohol content histogram. Recall that we are working with a numpy array and will need to use a matplotlib function to produce a histogram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "df.alcohol.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here:\n",
    "plt.hist(wine.data.T[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Clustering the Internal Dataset\n",
    "\n",
    "In this portion of the lab, we will cluster the data to find common traits between the different wines. We will use the k-means clustering algorithm to achieve this goal.\n",
    "\n",
    "#### We start by importing k-means from scikit-learn and then proceed to create 4 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "kmeans = KMeans(n_clusters=4).fit(wine.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 1, 0, 1, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 0, 0,\n",
       "       2, 2, 2, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0,\n",
       "       2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 1, 3, 0, 3, 0, 3, 3, 0,\n",
       "       3, 3, 0, 0, 2, 3, 3, 2, 2, 3, 3, 3, 0, 3, 3, 0, 0, 3, 3, 3, 3, 0,\n",
       "       0, 0, 3, 3, 3, 3, 3, 2, 0, 3, 0, 3, 0, 0, 3, 3, 0, 3, 3, 3, 3, 0,\n",
       "       0, 3, 0, 3, 3, 3, 3, 3, 3, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 3,\n",
       "       0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 0, 2, 2, 3, 0, 0, 0, 3, 3, 3, 0,\n",
       "       0, 0, 3, 2, 0, 0, 3, 0, 0, 0, 0, 3, 0, 0, 0, 0, 3, 3, 0, 0, 0, 2,\n",
       "       2, 0], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the size of each cluster. This can be done by counting the number of occurrences of each unique label in the list above.\n",
    "\n",
    "Which is the largest cluster of the 4?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 59, 1: 23, 2: 39, 3: 57}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "labels, counts = np.unique(kmeans.labels_, return_counts=True)\n",
    "dict(zip(labels, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your answer here:\n",
    "La clase 0 es la más grande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the shape of `wine['data']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 13)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "wine.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the first 5 records in `wine['data']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.23,  13.2 ,  13.16,  14.37,  13.24,  14.2 ,  14.39,  14.06,\n",
       "         14.83,  13.86,  14.1 ,  14.12,  13.75,  14.75,  14.38,  13.63,\n",
       "         14.3 ,  13.83,  14.19,  13.64,  14.06,  12.93,  13.71,  12.85,\n",
       "         13.5 ,  13.05,  13.39,  13.3 ,  13.87,  14.02,  13.73,  13.58,\n",
       "         13.68,  13.76,  13.51,  13.48,  13.28,  13.05,  13.07,  14.22,\n",
       "         13.56,  13.41,  13.88,  13.24,  13.05,  14.21,  14.38,  13.9 ,\n",
       "         14.1 ,  13.94,  13.05,  13.83,  13.82,  13.77,  13.74,  13.56,\n",
       "         14.22,  13.29,  13.72,  12.37,  12.33,  12.64,  13.67,  12.37,\n",
       "         12.17,  12.37,  13.11,  12.37,  13.34,  12.21,  12.29,  13.86,\n",
       "         13.49,  12.99,  11.96,  11.66,  13.03,  11.84,  12.33,  12.7 ,\n",
       "         12.  ,  12.72,  12.08,  13.05,  11.84,  12.67,  12.16,  11.65,\n",
       "         11.64,  12.08,  12.08,  12.  ,  12.69,  12.29,  11.62,  12.47,\n",
       "         11.81,  12.29,  12.37,  12.29,  12.08,  12.6 ,  12.34,  11.82,\n",
       "         12.51,  12.42,  12.25,  12.72,  12.22,  11.61,  11.46,  12.52,\n",
       "         11.76,  11.41,  12.08,  11.03,  11.82,  12.42,  12.77,  12.  ,\n",
       "         11.45,  11.56,  12.42,  13.05,  11.87,  12.07,  12.43,  11.79,\n",
       "         12.37,  12.04,  12.86,  12.88,  12.81,  12.7 ,  12.51,  12.6 ,\n",
       "         12.25,  12.53,  13.49,  12.84,  12.93,  13.36,  13.52,  13.62,\n",
       "         12.25,  13.16,  13.88,  12.87,  13.32,  13.08,  13.5 ,  12.79,\n",
       "         13.11,  13.23,  12.58,  13.17,  13.84,  12.45,  14.34,  13.48,\n",
       "         12.36,  13.69,  12.85,  12.96,  13.78,  13.73,  13.45,  12.82,\n",
       "         13.58,  13.4 ,  12.2 ,  12.77,  14.16,  13.71,  13.4 ,  13.27,\n",
       "         13.17,  14.13],\n",
       "       [  1.71,   1.78,   2.36,   1.95,   2.59,   1.76,   1.87,   2.15,\n",
       "          1.64,   1.35,   2.16,   1.48,   1.73,   1.73,   1.87,   1.81,\n",
       "          1.92,   1.57,   1.59,   3.1 ,   1.63,   3.8 ,   1.86,   1.6 ,\n",
       "          1.81,   2.05,   1.77,   1.72,   1.9 ,   1.68,   1.5 ,   1.66,\n",
       "          1.83,   1.53,   1.8 ,   1.81,   1.64,   1.65,   1.5 ,   3.99,\n",
       "          1.71,   3.84,   1.89,   3.98,   1.77,   4.04,   3.59,   1.68,\n",
       "          2.02,   1.73,   1.73,   1.65,   1.75,   1.9 ,   1.67,   1.73,\n",
       "          1.7 ,   1.97,   1.43,   0.94,   1.1 ,   1.36,   1.25,   1.13,\n",
       "          1.45,   1.21,   1.01,   1.17,   0.94,   1.19,   1.61,   1.51,\n",
       "          1.66,   1.67,   1.09,   1.88,   0.9 ,   2.89,   0.99,   3.87,\n",
       "          0.92,   1.81,   1.13,   3.86,   0.89,   0.98,   1.61,   1.67,\n",
       "          2.06,   1.33,   1.83,   1.51,   1.53,   2.83,   1.99,   1.52,\n",
       "          2.12,   1.41,   1.07,   3.17,   2.08,   1.34,   2.45,   1.72,\n",
       "          1.73,   2.55,   1.73,   1.75,   1.29,   1.35,   3.74,   2.43,\n",
       "          2.68,   0.74,   1.39,   1.51,   1.47,   1.61,   3.43,   3.43,\n",
       "          2.4 ,   2.05,   4.43,   5.8 ,   4.31,   2.16,   1.53,   2.13,\n",
       "          1.63,   4.3 ,   1.35,   2.99,   2.31,   3.55,   1.24,   2.46,\n",
       "          4.72,   5.51,   3.59,   2.96,   2.81,   2.56,   3.17,   4.95,\n",
       "          3.88,   3.57,   5.04,   4.61,   3.24,   3.9 ,   3.12,   2.67,\n",
       "          1.9 ,   3.3 ,   1.29,   5.19,   4.12,   3.03,   1.68,   1.67,\n",
       "          3.83,   3.26,   3.27,   3.45,   2.76,   4.36,   3.7 ,   3.37,\n",
       "          2.58,   4.6 ,   3.03,   2.39,   2.51,   5.65,   3.91,   4.28,\n",
       "          2.59,   4.1 ],\n",
       "       [  2.43,   2.14,   2.67,   2.5 ,   2.87,   2.45,   2.45,   2.61,\n",
       "          2.17,   2.27,   2.3 ,   2.32,   2.41,   2.39,   2.38,   2.7 ,\n",
       "          2.72,   2.62,   2.48,   2.56,   2.28,   2.65,   2.36,   2.52,\n",
       "          2.61,   3.22,   2.62,   2.14,   2.8 ,   2.21,   2.7 ,   2.36,\n",
       "          2.36,   2.7 ,   2.65,   2.41,   2.84,   2.55,   2.1 ,   2.51,\n",
       "          2.31,   2.12,   2.59,   2.29,   2.1 ,   2.44,   2.28,   2.12,\n",
       "          2.4 ,   2.27,   2.04,   2.6 ,   2.42,   2.68,   2.25,   2.46,\n",
       "          2.3 ,   2.68,   2.5 ,   1.36,   2.28,   2.02,   1.92,   2.16,\n",
       "          2.53,   2.56,   1.7 ,   1.92,   2.36,   1.75,   2.21,   2.67,\n",
       "          2.24,   2.6 ,   2.3 ,   1.92,   1.71,   2.23,   1.95,   2.4 ,\n",
       "          2.  ,   2.2 ,   2.51,   2.32,   2.58,   2.24,   2.31,   2.62,\n",
       "          2.46,   2.3 ,   2.32,   2.42,   2.26,   2.22,   2.28,   2.2 ,\n",
       "          2.74,   1.98,   2.1 ,   2.21,   1.7 ,   1.9 ,   2.46,   1.88,\n",
       "          1.98,   2.27,   2.12,   2.28,   1.94,   2.7 ,   1.82,   2.17,\n",
       "          2.92,   2.5 ,   2.5 ,   2.2 ,   1.99,   2.19,   1.98,   2.  ,\n",
       "          2.42,   3.23,   2.73,   2.13,   2.39,   2.17,   2.29,   2.78,\n",
       "          2.3 ,   2.38,   2.32,   2.4 ,   2.4 ,   2.36,   2.25,   2.2 ,\n",
       "          2.54,   2.64,   2.19,   2.61,   2.7 ,   2.35,   2.72,   2.35,\n",
       "          2.2 ,   2.15,   2.23,   2.48,   2.38,   2.36,   2.62,   2.48,\n",
       "          2.75,   2.28,   2.1 ,   2.32,   2.38,   2.64,   2.7 ,   2.64,\n",
       "          2.38,   2.54,   2.58,   2.35,   2.3 ,   2.26,   2.6 ,   2.3 ,\n",
       "          2.69,   2.86,   2.32,   2.28,   2.48,   2.45,   2.48,   2.26,\n",
       "          2.37,   2.74],\n",
       "       [ 15.6 ,  11.2 ,  18.6 ,  16.8 ,  21.  ,  15.2 ,  14.6 ,  17.6 ,\n",
       "         14.  ,  16.  ,  18.  ,  16.8 ,  16.  ,  11.4 ,  12.  ,  17.2 ,\n",
       "         20.  ,  20.  ,  16.5 ,  15.2 ,  16.  ,  18.6 ,  16.6 ,  17.8 ,\n",
       "         20.  ,  25.  ,  16.1 ,  17.  ,  19.4 ,  16.  ,  22.5 ,  19.1 ,\n",
       "         17.2 ,  19.5 ,  19.  ,  20.5 ,  15.5 ,  18.  ,  15.5 ,  13.2 ,\n",
       "         16.2 ,  18.8 ,  15.  ,  17.5 ,  17.  ,  18.9 ,  16.  ,  16.  ,\n",
       "         18.8 ,  17.4 ,  12.4 ,  17.2 ,  14.  ,  17.1 ,  16.4 ,  20.5 ,\n",
       "         16.3 ,  16.8 ,  16.7 ,  10.6 ,  16.  ,  16.8 ,  18.  ,  19.  ,\n",
       "         19.  ,  18.1 ,  15.  ,  19.6 ,  17.  ,  16.8 ,  20.4 ,  25.  ,\n",
       "         24.  ,  30.  ,  21.  ,  16.  ,  16.  ,  18.  ,  14.8 ,  23.  ,\n",
       "         19.  ,  18.8 ,  24.  ,  22.5 ,  18.  ,  18.  ,  22.8 ,  26.  ,\n",
       "         21.6 ,  23.6 ,  18.5 ,  22.  ,  20.7 ,  18.  ,  18.  ,  19.  ,\n",
       "         21.5 ,  16.  ,  18.5 ,  18.  ,  17.5 ,  18.5 ,  21.  ,  19.5 ,\n",
       "         20.5 ,  22.  ,  19.  ,  22.5 ,  19.  ,  20.  ,  19.5 ,  21.  ,\n",
       "         20.  ,  21.  ,  22.5 ,  21.5 ,  20.8 ,  22.5 ,  16.  ,  19.  ,\n",
       "         20.  ,  28.5 ,  26.5 ,  21.5 ,  21.  ,  21.  ,  21.5 ,  28.5 ,\n",
       "         24.5 ,  22.  ,  18.  ,  20.  ,  24.  ,  21.5 ,  17.5 ,  18.5 ,\n",
       "         21.  ,  25.  ,  19.5 ,  24.  ,  21.  ,  20.  ,  23.5 ,  20.  ,\n",
       "         18.5 ,  21.  ,  20.  ,  21.5 ,  21.5 ,  21.5 ,  24.  ,  22.  ,\n",
       "         25.5 ,  18.5 ,  20.  ,  22.  ,  19.5 ,  27.  ,  25.  ,  22.5 ,\n",
       "         21.  ,  20.  ,  22.  ,  18.5 ,  22.  ,  22.5 ,  23.  ,  19.5 ,\n",
       "         24.5 ,  25.  ,  19.  ,  19.5 ,  20.  ,  20.5 ,  23.  ,  20.  ,\n",
       "         20.  ,  24.5 ],\n",
       "       [127.  , 100.  , 101.  , 113.  , 118.  , 112.  ,  96.  , 121.  ,\n",
       "         97.  ,  98.  , 105.  ,  95.  ,  89.  ,  91.  , 102.  , 112.  ,\n",
       "        120.  , 115.  , 108.  , 116.  , 126.  , 102.  , 101.  ,  95.  ,\n",
       "         96.  , 124.  ,  93.  ,  94.  , 107.  ,  96.  , 101.  , 106.  ,\n",
       "        104.  , 132.  , 110.  , 100.  , 110.  ,  98.  ,  98.  , 128.  ,\n",
       "        117.  ,  90.  , 101.  , 103.  , 107.  , 111.  , 102.  , 101.  ,\n",
       "        103.  , 108.  ,  92.  ,  94.  , 111.  , 115.  , 118.  , 116.  ,\n",
       "        118.  , 102.  , 108.  ,  88.  , 101.  , 100.  ,  94.  ,  87.  ,\n",
       "        104.  ,  98.  ,  78.  ,  78.  , 110.  , 151.  , 103.  ,  86.  ,\n",
       "         87.  , 139.  , 101.  ,  97.  ,  86.  , 112.  , 136.  , 101.  ,\n",
       "         86.  ,  86.  ,  78.  ,  85.  ,  94.  ,  99.  ,  90.  ,  88.  ,\n",
       "         84.  ,  70.  ,  81.  ,  86.  ,  80.  ,  88.  ,  98.  , 162.  ,\n",
       "        134.  ,  85.  ,  88.  ,  88.  ,  97.  ,  88.  ,  98.  ,  86.  ,\n",
       "         85.  ,  90.  ,  80.  ,  84.  ,  92.  ,  94.  , 107.  ,  88.  ,\n",
       "        103.  ,  88.  ,  84.  ,  85.  ,  86.  , 108.  ,  80.  ,  87.  ,\n",
       "         96.  , 119.  , 102.  ,  86.  ,  82.  ,  85.  ,  86.  ,  92.  ,\n",
       "         88.  ,  80.  , 122.  , 104.  ,  98.  , 106.  ,  85.  ,  94.  ,\n",
       "         89.  ,  96.  ,  88.  , 101.  ,  96.  ,  89.  ,  97.  ,  92.  ,\n",
       "        112.  , 102.  ,  80.  ,  86.  ,  92.  , 113.  , 123.  , 112.  ,\n",
       "        116.  ,  98.  , 103.  ,  93.  ,  89.  ,  97.  ,  98.  ,  89.  ,\n",
       "         88.  , 107.  , 106.  , 106.  ,  90.  ,  88.  , 111.  ,  88.  ,\n",
       "        105.  , 112.  ,  96.  ,  86.  ,  91.  ,  95.  , 102.  , 120.  ,\n",
       "        120.  ,  96.  ]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "wine.data.T[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know the data object is a 2-dimensional array in which there are 178 rows and 13 columns. Each row is a data record and each column is a feature.\n",
    "\n",
    "#### What is the average ash content for each cluster? \n",
    "\n",
    "*Hints:* \n",
    "\n",
    "* *Ash* is the 3rd column.\n",
    "\n",
    "* The data object is not a Pandas dataframe so you can't apply `pandas.DataFrame.groupby`. Instead, you can use `np.average`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3665168539325845"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "np.average(wine.data.T[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Load and Explore an External Dataset\n",
    "\n",
    "We will now load an external dataset using Pandas and use scikit learn to explore the data. In this portion of the lab, we will use a [patient dataset from Kaggle](https://www.kaggle.com/miles99/patient-admission-dataset-for-learning-data-mining). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = pd.read_csv('../patient-admission-dataset-for-learning-data-mining.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, print the first five rows of the data using the `head()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, print the column types and check which columns have been misclassified by pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can see that none of the date columns have been correctly classified. Also, some columns contain qualitative data that can be dropped.\n",
    "\n",
    "First, transform the `patient_dob` and `appointment_date` columns to datetime using the `pd.to_datetime` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, drop the `id`, `patient_name`, `patient_email`, `patient_nhs_number`, and `doctor_phone` columns. These are not quantitative columns and will not contribute to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we work on the missing data. Most ML algorithms will not perform as intended if there are missing data.\n",
    "\n",
    "In the cell below, count how many rows contain missing data in each column. You should see three columns contain missing data:\n",
    "\n",
    "* `doctor_name`: 58 missing data\n",
    "* `prescribed_medicines`: 488 missing data\n",
    "* `diagnosis`: 488 missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main issues are found in the `prescribed_medicines` and `diagnosis` columns. Can we simply drop these rows?\n",
    "\n",
    "The answer is not yet. Because when there are missing data in these columns, it doesn't mean the data records are broken. Instead, it means no medication was prescribed and no diagnosis was recorded. Therefore, once we fill in the missing data these columns will be fine. But we'll revisit these columns and decide whether we will eventually drop them when we look at how many unique values are there in these categorical columns.  \n",
    "\n",
    "For the `prescribed_medicines` column, fill the missing values with the value `no prescription`. For the `diagnosis` column, fill the missing values with `no diagnosis`.\n",
    "\n",
    "*Hint: Use [`pandas.DataFrame.fillna`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about `doctor_name`? Since a doctor visit without a doctor name might not be meaningful, we will drop these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another step in preprocessing that can be performed by scikit-learn is label encoding. \n",
    "\n",
    "We have 4 columns that are of `bool` type. We would like to convert them to an integer column containing either zero or one. We can do this using [scikit-learn's label encoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).\n",
    "\n",
    "In the cell below, import the label encoder and encode the 4 boolean columns (*patient_diabetic*, *patient_allergic*, *patient_show*, *is_regular_visit*) with `0` and `1`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the data dtypes to confirm those four `bool` columns are converted to `int64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The last step is to handle the `object` data.\n",
    "\n",
    "There are 4 `object` columns now: `patient_gender`, `doctor_name`, `prescribed_medicines`, and `diagnosis`. The gender columns\n",
    "\n",
    "In the next cell, check the unique values of each of the `object` columns using `value_counts()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of unique values is large for all three columns except `patient_gender`. We will handle these columns differently.\n",
    "\n",
    "For `diagnosis`, there are too many unique values which will make ML difficult. However, we can re-encode the values to either with or without diagnosis. Remember at an earlier step we filled in the missing values of this column with *no diagnosis*? We can re-encode *no diagnosis* to `0` and all other values to `1`. In this way we can tremendously simply this column.\n",
    "\n",
    "For `prescribed_medicines`, we can drop this column because it is perfectly correlated with `diagnosis`. Whenever there is no diagnosis, there is no prescribed medicine. So we don't need to keep this duplicated data.\n",
    "\n",
    "How about `doctor_name`? There are not excessive unique values but still quite many (19). We may either drop or keep it but keeping it will make the analysis more complicated. So due to the length of this lab let's drop it.\n",
    "\n",
    "How about `gender`? This one is easy. Just like re-encoding the boolean values, we can re-encode gender to `0` and `1` because there are only 2 unique values.\n",
    "\n",
    "In the next cells, do the following:\n",
    "\n",
    "1. Create a new column called `diagnosis_int` that has `0` and `1` based on the values in `diagnosis`.\n",
    "\n",
    "1. Create a new column called `patient_gender_int` that has `0` and `1` based on the values in `patient_gender`.\n",
    "\n",
    "1. Drop the following columns: `doctor_name`, `diagnosis`, `prescribed_medicines`, and `patient_gender`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the head again to ensure the re-encoding and dropping are successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting observation is that all patients are no older than 2 years. However, their weights and heights indicate that they are adults. This cannot be true. Therefore, we can either trust the weight and height columns or the DOB column. Since there are other columns that indicate that these are adults (they have emails, some have diabetes) we will drop the `patient_dob` column. We will also drop the `appointment_date` column since it has too many unique values to transform to a dummy variable. Drop the two columns in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our data is now ready for clustering. Let's use k-means again.\n",
    "\n",
    "We start by initializing and fitting a model in the cell below. Call this model patients_cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach the labels to the dataframe. Do this by accessing the `labels_` in the `patients_cluster` model and assign them to a new column in `patients` that you will call `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using a `groupby`, find the mean of every variable in `patients` and group by the `labels` column. This summary will allow us to see how the patients differ between the clusters. Your output should look similar to the image below.\n",
    "\n",
    "![groupby mean](../groupby-mean.png)\n",
    "\n",
    "Additionally, add a comment to describe which columns have the largest difference between clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your comment here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge: Visualize K-Means Clusters\n",
    "\n",
    "How did k-means cluster the data? You can obtain an intuitive view with a scatter plot. Generate a 2-d cluster plot below using `matplotlib`. You need to choose 2 of the features from your cleaned and transformed dataset, and use color to represent the cluster label generated from k-means.\n",
    "\n",
    "If the scatter plot does not make any sense to you, it means the features you chose to visualize are not the right ones. You should be able to see 4 clear clusters with different colors in your visualization that suggests how k-means had clustered your data.\n",
    "\n",
    "![Cluster Visualization](../clusters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can visualize the clusters in 3-D scatter plot. Give it a try below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
